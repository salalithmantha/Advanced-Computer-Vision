{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=1\n",
    "epc=1\n",
    "loss_graph_list=[]\n",
    "\n",
    "\n",
    "#Reading the data\n",
    "def unpickle(file):\n",
    "\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "train=unpickle(\"cifar-100-python/train\")\n",
    "test=unpickle(\"cifar-100-python/test\")\n",
    "\n",
    "print(train.keys())\n",
    "print(test.keys())\n",
    "\n",
    "#Train data\n",
    "Xtrain=train[b'data'].reshape((len(train[b'data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "ytrain=np.array(train[b'fine_labels'])\n",
    "\n",
    "#Test data\n",
    "Xtest=test[b'data'].reshape((len(test[b'data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "ytest=np.array(test[b'fine_labels'])\n",
    "\n",
    "Xtrain=Xtrain.astype(np.float32)\n",
    "Xtest=Xtest.astype(np.float32)\n",
    "\n",
    "Xval=Xtrain[40000:,:,:,:]\n",
    "Xtrain=Xtrain[0:40000]\n",
    "\n",
    "yval=ytrain[40000:]\n",
    "ytrain=ytrain[0:40000]\n",
    "\n",
    "# Reset graph parameters\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input_x')\n",
    "y = tf.placeholder(tf.int32, shape=(None,), name='output_y')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model Architecture\n",
    "def leNet(features):\n",
    "    # layer 1 input\n",
    "    # input_layer=tf.reshape(features['x'],[-1,32,32,3])\n",
    "\n",
    "    # conv_layer #1\n",
    "    conv1 = tf.layers.conv2d(inputs=features, filters=64,\n",
    "                             kernel_size=[3,3], strides=1,\n",
    "                             activation=tf.nn.relu,\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    # pooling_layer #1\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1,\n",
    "                                    pool_size=[2, 2],\n",
    "                                    strides=2)\n",
    "\n",
    "    batch1=tf.layers.batch_normalization(inputs=pool1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # conv_layer #2\n",
    "    conv2 = tf.layers.conv2d(inputs=batch1, filters=128,\n",
    "                             kernel_size=[3,3], strides=1,\n",
    "                             activation=tf.nn.relu,\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    # pooling_layer #2\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2,\n",
    "                                    pool_size=[2, 2],\n",
    "                                    strides=2)\n",
    "\n",
    "    batch2=tf.layers.batch_normalization(inputs=pool2)\n",
    "\n",
    "\n",
    "\n",
    "    # Dense_layer #1\n",
    "    pool2_flat = tf.reshape(batch2, [-1, 4608])\n",
    "    dense1 = tf.layers.dense(inputs=pool2_flat, units=256,\n",
    "                             activation=tf.nn.relu,\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    batch3=tf.layers.batch_normalization(inputs=dense1)\n",
    "\n",
    "\n",
    "    # Dense_layer #2\n",
    "    dense2 = tf.layers.dense(inputs=batch3, units=512,\n",
    "                             activation=tf.nn.relu,\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    batch4=tf.layers.batch_normalization(inputs=dense2)\n",
    "\n",
    "    # logits_final_layer\n",
    "    logits = tf.layers.dense(inputs=batch4, units=100)\n",
    "\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epochs, batchSize,Learning rate\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Output of model\n",
    "logits = leNet(x)\n",
    "model = tf.identity(logits, name='logits')\n",
    "\n",
    "#Loss function & Optimization Algorithm\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "#Prediction and Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1),tf.cast(y,tf.int64))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling Data\n",
    "def batch_features_labels(features, labels, batch_size):\n",
    "\n",
    "    rand_index=np.random.choice(len(features),size=len(features))\n",
    "    for start in range(0,len(features),batch_size):\n",
    "        end=min(start+batch_size,len(features))\n",
    "        tmp=np.array(rand_index[start:end])\n",
    "        yield features[tmp],labels[tmp]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#saving model\n",
    "model_path = './image_classification'\n",
    "\n",
    "\n",
    "#Creating Session\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    #inistalizing Global_variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    ###########\n",
    "\n",
    "    # Data Augmentation\n",
    "    data_tf=tf.convert_to_tensor(Xtrain,np.float32)\n",
    "    big=tf.image.resize_images(Xtrain,(36,36))\n",
    "    top_r=tf.image.crop_to_bounding_box(big,0,0,32,32)\n",
    "    top_l=tf.image.crop_to_bounding_box(big,0,4,32,32)\n",
    "    bot_r=tf.image.crop_to_bounding_box(big,4,0,32,32)\n",
    "    bot_l=tf.image.crop_to_bounding_box(big, 4, 4, 32, 32)\n",
    "    cen=tf.image.crop_to_bounding_box(big,2,2,32,32)\n",
    "    flip=tf.image.flip_left_right(data_tf)\n",
    "\n",
    "    fbig = tf.image.resize_images(flip, (36, 36))\n",
    "    ftop_r = tf.image.crop_to_bounding_box(fbig, 0, 0, 32, 32)\n",
    "    ftop_l = tf.image.crop_to_bounding_box(fbig, 0, 4, 32, 32)\n",
    "    fbot_r = tf.image.crop_to_bounding_box(fbig, 4, 0, 32, 32)\n",
    "    fbot_l = tf.image.crop_to_bounding_box(fbig, 4, 4, 32, 32)\n",
    "    fcen = tf.image.crop_to_bounding_box(fbig, 2, 2, 32, 32)\n",
    "\n",
    "\n",
    "    Xtrain1 = tf.concat([data_tf, top_r, top_l, bot_l, bot_r, cen, flip], axis=0)\n",
    "    Xtrain2=tf.concat([ftop_l,ftop_r,fbot_l,fbot_r,fcen],axis=0)\n",
    "\n",
    "    # Xtrain=tf.concat([Xtrain1,Xtrain2],axis=0)\n",
    "\n",
    "    sess.run(Xtrain1)\n",
    "    sess.run(Xtrain2)\n",
    "    Xtrain1=Xtrain1.eval()\n",
    "    Xtrain2=Xtrain2.eval()\n",
    "    Xtrain=np.concatenate((Xtrain1,Xtrain2))\n",
    "    ytrain=np.concatenate((ytrain,ytrain,ytrain,ytrain,ytrain,ytrain,ytrain,ytrain,ytrain,ytrain,ytrain,ytrain))\n",
    "    # Xtrain=Xtrain.eval()\n",
    "    print(type(Xtrain))\n",
    "    print(Xtrain.shape)\n",
    "    print(ytrain.shape)\n",
    "\n",
    "    #Calculating mean of the dataSet\n",
    "    sub = np.mean(Xtrain, axis=0)\n",
    "    Xtrain = Xtrain - sub\n",
    "    Xval=Xval-sub\n",
    "    np.save(\"mean_vec\",sub)\n",
    "\n",
    "    # Traning and Validation\n",
    "    for epoch in range(0,epochs):\n",
    "        for batch_features, batch_labels in batch_features_labels(Xtrain, ytrain, batch_size):\n",
    "            sess.run(optimizer,\n",
    "                            feed_dict={\n",
    "                                x: batch_features,\n",
    "                                y: batch_labels\n",
    "                            })\n",
    "\n",
    "            loss = sess.run(cost,\n",
    "                            feed_dict={\n",
    "                                x: batch_features,\n",
    "                                y: batch_labels\n",
    "                            })\n",
    "            loss_graph_list.append(loss)\n",
    "\n",
    "            if(epoch==epc):\n",
    "                epc+=1\n",
    "                print('Epoch {:>2}\\n'.format(epoch), end='')\n",
    "                print(sum(loss_graph_list)/len(loss_graph_list))\n",
    "\n",
    "            if(epoch==count):\n",
    "                count+=1\n",
    "                valid_acc = sess.run(accuracy,\n",
    "                                     feed_dict={\n",
    "                                         x: Xval,\n",
    "                                         y: yval})\n",
    "\n",
    "                print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss, valid_acc))\n",
    "\n",
    "\n",
    "    # Saving model and plotting Loss Graph\n",
    "    plt.plot(loss_graph_list)\n",
    "    plt.show()\n",
    "    plt.savefig(\"myfig\")\n",
    "    model_saver = tf.train.Saver()\n",
    "    save_path_model = model_saver.save(sess, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
